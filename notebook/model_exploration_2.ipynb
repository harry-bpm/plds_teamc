{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import sklearn\n",
    "import joblib\n",
    "import time\n",
    "from string import punctuation\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, f1_score, plot_roc_curve, make_scorer\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import lightgbm as lgb\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_knn(class_weight = None):\n",
    "    \"\"\"\n",
    "    Function for initiating Logistic Regression Model\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    param_dist = {'n_neighbors' : [10, 20, 30]}\n",
    "    base_model = KNeighborsClassifier(random_state=42)\n",
    "\n",
    "    \n",
    "    return param_dist, base_model\n",
    "\n",
    "def model_dt(class_weight = None):\n",
    "    \"\"\"\n",
    "    Function for initiating Random Forest Model\n",
    "    \"\"\"\n",
    "    \n",
    "    param_dist = None\n",
    "    base_model = DecisionTreeClassifier(random_state=42)\n",
    "    \n",
    "    return param_dist, base_model\n",
    "\n",
    "def model_mlp(class_weight = None):\n",
    "    \"\"\"\n",
    "    Function for initiating LightGBM Model\n",
    "    \"\"\"\n",
    "\n",
    "    param_dist = {'hidden_layer_sizes' : [(150,100,50), (100,), (150,100)]}\n",
    "    base_model = MLPClassifier(random_state=42, max_iter=300)\n",
    "    \n",
    "    return param_dist, base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search_cv(model, param, scoring, n_iter, x, y, verbosity=0):\n",
    "    \"\"\"\n",
    "    Just a function to run the hyperparameter search\n",
    "    \"\"\"\n",
    "    random_fit = RandomizedSearchCV(estimator = model, \n",
    "                                    param_distributions = param, \n",
    "                                    scoring = scoring, \n",
    "                                    n_iter = n_iter, \n",
    "                                    cv = 5, \n",
    "                                    random_state = 42, \n",
    "                                    verbose = verbosity)\n",
    "    random_fit.fit(x, y)\n",
    "    return random_fit\n",
    "\n",
    "def calibrate_classifier(model, x_valid, y_valid):\n",
    "    model_calibrated = CalibratedClassifierCV(model, cv='prefit')\n",
    "    model_calibrated.fit(x_valid, y_valid)\n",
    "    \n",
    "    return model_calibrated\n",
    "\n",
    "\n",
    "def tune_threshold(model, x_valid, y_valid, scorer):\n",
    "    \"\"\"\n",
    "    Function for threshold adjustment\n",
    "    \n",
    "    Args:\n",
    "        - model(callable): Sklearn model\n",
    "        - x_valid(DataFrame):\n",
    "        - y_valid(DataFrame):\n",
    "        - scorer(callable): Sklearn scorer function, for example: f1_score\n",
    "    \"\"\"\n",
    "    thresholds = np.linspace(0,1,101)\n",
    "    proba = model.predict_proba(x_valid)[:, 1]\n",
    "    proba = pd.DataFrame(proba)\n",
    "    proba.columns = ['probability']\n",
    "    score = []\n",
    "    for threshold_value in thresholds:\n",
    "        proba['prediction'] = np.where( proba['probability'] > threshold_value, 1, 0)\n",
    "        metric_score = scorer(proba['prediction'], y_valid, average='macro')\n",
    "        score.append(metric_score)\n",
    "    metric_score = pd.DataFrame([thresholds,score]).T\n",
    "    metric_score.columns = ['threshold','metric_score']\n",
    "    best_score = (metric_score['metric_score'] == metric_score['metric_score'].max())\n",
    "    best_threshold = metric_score[best_score]['threshold']\n",
    "    \n",
    "    return metric_score[\"metric_score\"].max(), best_threshold.values[0]\n",
    "\n",
    "def select_model(train_log_dict):\n",
    "    max_score = max(train_log_dict['model_score'])\n",
    "    max_index = train_log_dict['model_score'].index(max_score)\n",
    "    best_model = train_log_dict['model_fit'][max_index]\n",
    "    best_report = train_log_dict['model_report'][max_index]\n",
    "    best_threshold = train_log_dict['threshold'][max_index]\n",
    "    name = train_log_dict['model_name'][max_index]\n",
    "\n",
    "    return best_model, best_report, best_threshold, name\n",
    "\n",
    "\n",
    "\n",
    "def classif_report(model_obj, x_test, y_test, best_threshold=None, calc_auc=True):\n",
    "    code2rel = {'0': 'Non-Toxic', '1': 'Toxic'}\n",
    "    \n",
    "    if best_threshold is None:\n",
    "        pred = model_obj.predict(x_test)\n",
    "    else:\n",
    "        proba = model_obj.predict_proba(x_test)[:, 1]\n",
    "        pred = np.where(proba > best_threshold, 1, 0)\n",
    "\n",
    "    res = classification_report(\n",
    "        y_test, pred, output_dict=True, zero_division=0)\n",
    "    res = pd.DataFrame(res).rename(columns=code2rel).T\n",
    "\n",
    "    if calc_auc:\n",
    "        proba = model_obj.predict_proba(x_test)[:, 1]\n",
    "        auc_score = roc_auc_score(y_test, proba)\n",
    "\n",
    "        print(\n",
    "            f\"AUC score: {auc_score}, F1-Macro: {res['f1-score']['macro avg']}\")\n",
    "    return pred, res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(x_train, y_train, model, model_param, scoring='f1', n_iter=3, verbosity=3):\n",
    "    \"\"\"\n",
    "    Fit model\n",
    "    \n",
    "    Args:\n",
    "        - model(callable): sklearn model\n",
    "        - model_param(dict): sklearn's RandomizedSearchCV params_distribution\n",
    "    \n",
    "    Return:\n",
    "        - model_fitted(callable): model with optimum hyperparams\n",
    "    \"\"\"\n",
    "    model_fitted = random_search_cv(model, model_param, \n",
    "                                    scoring, \n",
    "                                    n_iter, \n",
    "                                    x_train, y_train, \n",
    "                                    verbosity)\n",
    "    print(\n",
    "        f'Model: {model_fitted.best_estimator_}, {scoring}: {model_fitted.best_score_}')\n",
    "    \n",
    "    return model_fitted\n",
    "\n",
    "def validate(x_valid, y_valid, model_fitted, tune = True):\n",
    "    \"\"\"\n",
    "    Validate model\n",
    "\n",
    "    Args:\n",
    "        - x_valid(DataFrame): Validation independent variables\n",
    "        - y_valid(DataFrame): Validation Dependent variables\n",
    "        - model_fitted(callable): Sklearn / imblearn fitted model\n",
    "    \"\"\"\n",
    "    code2rel = {'0': 'Non-Toxic', '1': 'Toxic'}\n",
    "\n",
    "    # Calibrate Classifier\n",
    "    model_calibrated = CalibratedClassifierCV(base_estimator=model_fitted,\n",
    "                                              cv=\"prefit\")\n",
    "    model_calibrated.fit(x_valid, y_valid)\n",
    "    \n",
    "    if tune:\n",
    "        metric_score, best_threshold = tune_threshold(model_calibrated,\n",
    "                                                      x_valid,\n",
    "                                                      y_valid,\n",
    "                                                      f1_score)\n",
    "        \n",
    "        print(f'Best threshold is: {best_threshold}, with score: {metric_score}')\n",
    "        pred_model, report_model = classif_report(model_calibrated,\n",
    "                                                  x_valid,\n",
    "                                                  y_valid,\n",
    "                                                  best_threshold,\n",
    "                                                  True)\n",
    "    else:\n",
    "        # Report default\n",
    "        best_threshold = None\n",
    "        pred_model, report_model = classif_report(\n",
    "            model_calibrated, x_valid, y_valid, True)\n",
    "\n",
    "    return report_model, model_calibrated, best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(x_train, y_train, x_valid, y_valid):\n",
    "    \n",
    "    x_train = x_train.drop(columns='id')\n",
    "    y_train = y_train.drop(columns='id')\n",
    "    x_valid = x_valid.drop(columns='id')\n",
    "    y_valid = y_valid.drop(columns='id')\n",
    "    \n",
    "    y_train = y_train.values.ravel()\n",
    "    y_valid = y_valid.values.ravel()\n",
    "\n",
    "    # Add class weight\n",
    "    class_weight = compute_class_weight(class_weight = 'balanced', \n",
    "                                        classes = np.unique(y_train), \n",
    "                                        y = y_train)\n",
    "    class_weights = dict(zip(np.unique(y_train), class_weight))\n",
    "    \n",
    "    # Initiate models\n",
    "    knn = model_knn\n",
    "    dt = model_dt\n",
    "    mlp = model_mlp\n",
    "    \n",
    "    # Initiate logs\n",
    "    train_log_dict = {'model': [knn, dt, mlp],\n",
    "                      'model_name': [],\n",
    "                      'model_fit': [],\n",
    "                      'model_report': [],\n",
    "                      'model_score': [],\n",
    "                      'threshold': [],\n",
    "                      'fit_time': []}\n",
    "\n",
    "\n",
    "    # Try Each models\n",
    "    for model in train_log_dict['model']:\n",
    "        param_model, base_model = model(class_weights)\n",
    "        train_log_dict['model_name'].append(base_model.__class__.__name__)\n",
    "        print(f'Fitting {base_model.__class__.__name__}')\n",
    "\n",
    "        # Train\n",
    "        t0 = time.time()\n",
    "        scoring = make_scorer(f1_score,average='macro')\n",
    "        fitted_model = fit(\n",
    "            x_train, y_train, base_model, param_model, scoring=scoring)\n",
    "        elapsed_time = time.time() - t0\n",
    "        print(f'elapsed time: {elapsed_time} s \\n')\n",
    "        train_log_dict['fit_time'].append(elapsed_time)\n",
    "\n",
    "        # Validate\n",
    "        report, calibrated_model, best_threshold = validate(\n",
    "            x_valid, y_valid, fitted_model)\n",
    "        train_log_dict['model_fit'].append(calibrated_model)\n",
    "        train_log_dict['threshold'].append(best_threshold)\n",
    "        train_log_dict['model_report'].append(report)\n",
    "        train_log_dict['model_score'].append(report['f1-score']['macro avg'])\n",
    "\n",
    "    best_model, best_report, best_threshold, name = select_model(\n",
    "        train_log_dict)\n",
    "    print(\n",
    "        f\"Model: {name}, Score: {best_report['f1-score']['macro avg']}\")\n",
    "    joblib.dump(best_model, 'E:\\\\projects\\\\plds_latihan\\\\pipeline\\\\mantab_model.pkl')\n",
    "    joblib.dump(best_threshold, 'E:\\\\projects\\\\plds_latihan\\\\pipeline\\\\threshold.pkl')\n",
    "    joblib.dump(train_log_dict, 'E:\\\\projects\\\\plds_latihan\\\\pipeline\\\\train_log.pkl')\n",
    "    print(f'\\n {best_report}')\n",
    "    \n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = main(x_train_vect, y_train, x_valid_vect, y_valid)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
