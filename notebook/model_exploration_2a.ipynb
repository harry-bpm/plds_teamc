{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import sklearn\n",
    "import joblib\n",
    "import time\n",
    "from string import punctuation\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, f1_score, plot_roc_curve, make_scorer\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import lightgbm as lgb\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_knn():\n",
    "    \"\"\"\n",
    "    Function for initiating Logistic Regression Model\n",
    "    \"\"\"\n",
    "\n",
    "    base_model = KNeighborsClassifier()\n",
    "\n",
    "    \n",
    "    return base_model\n",
    "\n",
    "def model_dt():\n",
    "    \"\"\"\n",
    "    Function for initiating Random Forest Model\n",
    "    \"\"\"\n",
    "    \n",
    "    base_model = DecisionTreeClassifier(random_state=42)\n",
    "    \n",
    "    return base_model\n",
    "\n",
    "def model_mlp():\n",
    "    \"\"\"\n",
    "    Function for initiating LightGBM Model\n",
    "    \"\"\"\n",
    "\n",
    "    base_model = MLPClassifier(random_state=42, max_iter=300)\n",
    "    \n",
    "    return base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def select_model(train_log_dict):\n",
    "    max_score = max(train_log_dict['model_score'])\n",
    "    max_index = train_log_dict['model_score'].index(max_score)\n",
    "    best_model = train_log_dict['model_fit'][max_index]\n",
    "    name = train_log_dict['model_name'][max_index]\n",
    "\n",
    "    return best_model, name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X_train, Y_train, model):\n",
    "\n",
    "    model_fitted = model.fit(X_train, Y_train)\n",
    "    return model_fitted\n",
    "\n",
    "def validate(X_test, Y_test, model_fitted, ):\n",
    "\n",
    "    auc_score = roc_auc_score(Y_test, model_fitted.predict_proba(X_test))\n",
    "\n",
    "\n",
    "    return auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(X_train, X_test, Y_train,  Y_test):\n",
    "    \n",
    "\n",
    "    # Initiate models\n",
    "    knn = model_knn\n",
    "    dt = model_dt\n",
    "    mlp = model_mlp\n",
    "    \n",
    "    # Initiate logs\n",
    "    train_log_dict = {'model': [knn, dt, mlp],\n",
    "                      'model_name': [],\n",
    "                      'model_score': []}\n",
    "\n",
    "\n",
    "    # Try Each models\n",
    "    for model in train_log_dict['model']:\n",
    "        base_model = model()\n",
    "        train_log_dict['model_name'].append(base_model.__class__.__name__)\n",
    "        print(f'Fitting {base_model.__class__.__name__}')\n",
    "\n",
    "        # Train\n",
    "        fitted_model = fit(X_train, Y_train, base_model)\n",
    "\n",
    "        # Validate\n",
    "        score = validate(X_test, Y_test, fitted_model)\n",
    "        train_log_dict['model_score'].append(score)\n",
    "\n",
    "    best_model, best_report, best_threshold, name = select_model(train_log_dict)\n",
    "    \n",
    "    print(\n",
    "        f\"Model: {name}, Score: {best_report['f1-score']['macro avg']}\")\n",
    "    joblib.dump(best_model, 'E:\\\\projects\\\\plds_latihan\\\\pipeline\\\\mantab_model.pkl')\n",
    "    joblib.dump(best_threshold, 'E:\\\\projects\\\\plds_latihan\\\\pipeline\\\\threshold.pkl')\n",
    "    joblib.dump(train_log_dict, 'E:\\\\projects\\\\plds_latihan\\\\pipeline\\\\train_log.pkl')\n",
    "    print(f'\\n {best_report}')\n",
    "    \n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "\n",
    "    X_train = joblib.load(\"E:\\\\projects\\\\plds_teamc\\\\data\\\\X_train.pkl\")\n",
    "    X_test = joblib.load(\"E:\\\\projects\\\\plds_teamc\\\\data\\\\X_test.pkl\")\n",
    "    Y_train = joblib.load(\"E:\\\\projects\\\\plds_teamc\\\\data\\\\Y_train.pkl\")\n",
    "    Y_test = joblib.load(\"E:\\\\projects\\\\plds_teamc\\\\data\\\\Y_test.pkl\")\n",
    "\n",
    "    return X_train, X_test, Y_train,  Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train,  Y_test= load_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build VT-CNN2 Neural Net model using Keras primitives -- \n",
    "#  - Reshape [N,2,128] to [N,1,2,128] on input\n",
    "#  - Pass through 2 2DConv/ReLu layers\n",
    "#  - Pass through 2 Dense layers (ReLu and Softmax)\n",
    "#  - Perform categorical cross entropy optimization\n",
    "import tensorflow.keras.models as models\n",
    "import tensorflow.keras.callbacks\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout\n",
    "\n",
    "from tensorflow.keras.layers import PReLU\n",
    "from tensorflow.keras.layers import MaxPooling2D \n",
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "model = models.Sequential()\n",
    "#model.add(Reshape([1]+in_shp, input_shape=in_shp))\n",
    "model.add(Dense(32, name=\"dense1\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Dense(64, name=\"dense2\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(128, name=\"dense3\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Dense(256, name=\"dense4\", kernel_initializer=\"he_normal\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(512, name=\"dense5\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(256, name=\"dense6\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Dense(128, name=\"dense7\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64, name=\"dense8\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Dense(32, name=\"dense9\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Dense(1, name=\"dense10\", kernel_initializer=\"he_normal\", activation = 'sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.asarray(X_train).astype(np.float32)\n",
    "X_test = np.asarray(X_test).astype(np.float32)\n",
    "Y_train = np.asarray(Y_train).astype(np.float32)\n",
    "Y_test = np.asarray(Y_test).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "456/456 - 9s - loss: 0.6545 - accuracy: 0.7230 - val_loss: 0.5147 - val_accuracy: 0.7402\n",
      "Epoch 2/50\n",
      "456/456 - 9s - loss: 0.5175 - accuracy: 0.7394 - val_loss: 0.5109 - val_accuracy: 0.7460\n",
      "Epoch 3/50\n",
      "456/456 - 9s - loss: 0.5154 - accuracy: 0.7401 - val_loss: 0.5096 - val_accuracy: 0.7440\n",
      "Epoch 4/50\n",
      "456/456 - 9s - loss: 0.5148 - accuracy: 0.7402 - val_loss: 0.5094 - val_accuracy: 0.7388\n",
      "Epoch 5/50\n",
      "456/456 - 8s - loss: 0.5138 - accuracy: 0.7410 - val_loss: 0.5105 - val_accuracy: 0.7464\n",
      "Epoch 6/50\n",
      "456/456 - 8s - loss: 0.5134 - accuracy: 0.7409 - val_loss: 0.5105 - val_accuracy: 0.7461\n",
      "Epoch 7/50\n",
      "456/456 - 8s - loss: 0.5133 - accuracy: 0.7409 - val_loss: 0.5111 - val_accuracy: 0.7470\n",
      "Epoch 8/50\n",
      "456/456 - 9s - loss: 0.5128 - accuracy: 0.7414 - val_loss: 0.5104 - val_accuracy: 0.7453\n",
      "Epoch 9/50\n",
      "456/456 - 9s - loss: 0.5126 - accuracy: 0.7415 - val_loss: 0.5088 - val_accuracy: 0.7449\n",
      "Epoch 10/50\n",
      "456/456 - 9s - loss: 0.5125 - accuracy: 0.7412 - val_loss: 0.5097 - val_accuracy: 0.7369\n",
      "Epoch 11/50\n",
      "456/456 - 9s - loss: 0.5125 - accuracy: 0.7417 - val_loss: 0.5098 - val_accuracy: 0.7459\n",
      "Epoch 12/50\n",
      "456/456 - 8s - loss: 0.5124 - accuracy: 0.7413 - val_loss: 0.5087 - val_accuracy: 0.7439\n",
      "Epoch 13/50\n",
      "456/456 - 8s - loss: 0.5121 - accuracy: 0.7420 - val_loss: 0.5141 - val_accuracy: 0.7302\n",
      "Epoch 14/50\n",
      "456/456 - 8s - loss: 0.5125 - accuracy: 0.7421 - val_loss: 0.5102 - val_accuracy: 0.7398\n",
      "Epoch 15/50\n",
      "456/456 - 8s - loss: 0.5117 - accuracy: 0.7418 - val_loss: 0.5109 - val_accuracy: 0.7394\n",
      "Epoch 16/50\n",
      "456/456 - 8s - loss: 0.5119 - accuracy: 0.7414 - val_loss: 0.5111 - val_accuracy: 0.7340\n",
      "Epoch 17/50\n",
      "456/456 - 8s - loss: 0.5118 - accuracy: 0.7415 - val_loss: 0.5106 - val_accuracy: 0.7328\n",
      "Epoch 18/50\n",
      "456/456 - 8s - loss: 0.5119 - accuracy: 0.7415 - val_loss: 0.5097 - val_accuracy: 0.7364\n",
      "Epoch 19/50\n",
      "456/456 - 8s - loss: 0.5115 - accuracy: 0.7416 - val_loss: 0.5111 - val_accuracy: 0.7485\n",
      "Epoch 20/50\n",
      "456/456 - 8s - loss: 0.5115 - accuracy: 0.7416 - val_loss: 0.5086 - val_accuracy: 0.7422\n",
      "Epoch 21/50\n",
      "456/456 - 9s - loss: 0.5114 - accuracy: 0.7420 - val_loss: 0.5091 - val_accuracy: 0.7411\n",
      "Epoch 22/50\n",
      "456/456 - 8s - loss: 0.5116 - accuracy: 0.7419 - val_loss: 0.5090 - val_accuracy: 0.7463\n",
      "Epoch 23/50\n",
      "456/456 - 8s - loss: 0.5114 - accuracy: 0.7417 - val_loss: 0.5092 - val_accuracy: 0.7452\n",
      "Epoch 24/50\n",
      "456/456 - 8s - loss: 0.5113 - accuracy: 0.7418 - val_loss: 0.5090 - val_accuracy: 0.7427\n",
      "Epoch 25/50\n",
      "456/456 - 8s - loss: 0.5111 - accuracy: 0.7423 - val_loss: 0.5111 - val_accuracy: 0.7480\n",
      "Epoch 26/50\n",
      "456/456 - 8s - loss: 0.5117 - accuracy: 0.7418 - val_loss: 0.5100 - val_accuracy: 0.7433\n",
      "Epoch 27/50\n",
      "456/456 - 8s - loss: 0.5110 - accuracy: 0.7419 - val_loss: 0.5119 - val_accuracy: 0.7489\n",
      "Epoch 28/50\n",
      "456/456 - 9s - loss: 0.5114 - accuracy: 0.7426 - val_loss: 0.5093 - val_accuracy: 0.7378\n",
      "Epoch 29/50\n",
      "456/456 - 8s - loss: 0.5111 - accuracy: 0.7423 - val_loss: 0.5108 - val_accuracy: 0.7483\n",
      "Epoch 30/50\n",
      "456/456 - 8s - loss: 0.5111 - accuracy: 0.7423 - val_loss: 0.5096 - val_accuracy: 0.7361\n",
      "Epoch 31/50\n",
      "456/456 - 8s - loss: 0.5111 - accuracy: 0.7420 - val_loss: 0.5086 - val_accuracy: 0.7423\n",
      "Epoch 32/50\n",
      "456/456 - 8s - loss: 0.5110 - accuracy: 0.7422 - val_loss: 0.5089 - val_accuracy: 0.7437\n",
      "Epoch 33/50\n",
      "456/456 - 8s - loss: 0.5109 - accuracy: 0.7428 - val_loss: 0.5121 - val_accuracy: 0.7322\n",
      "Epoch 34/50\n",
      "456/456 - 8s - loss: 0.5113 - accuracy: 0.7421 - val_loss: 0.5090 - val_accuracy: 0.7462\n",
      "Epoch 35/50\n",
      "456/456 - 8s - loss: 0.5108 - accuracy: 0.7425 - val_loss: 0.5093 - val_accuracy: 0.7380\n",
      "Epoch 36/50\n",
      "456/456 - 8s - loss: 0.5110 - accuracy: 0.7417 - val_loss: 0.5099 - val_accuracy: 0.7469\n",
      "Epoch 37/50\n",
      "456/456 - 9s - loss: 0.5109 - accuracy: 0.7424 - val_loss: 0.5093 - val_accuracy: 0.7382\n",
      "Epoch 38/50\n",
      "456/456 - 8s - loss: 0.5107 - accuracy: 0.7424 - val_loss: 0.5097 - val_accuracy: 0.7368\n",
      "Epoch 39/50\n",
      "456/456 - 8s - loss: 0.5107 - accuracy: 0.7427 - val_loss: 0.5145 - val_accuracy: 0.7236\n",
      "Epoch 40/50\n",
      "456/456 - 8s - loss: 0.5108 - accuracy: 0.7427 - val_loss: 0.5087 - val_accuracy: 0.7448\n",
      "Epoch 41/50\n",
      "456/456 - 9s - loss: 0.5105 - accuracy: 0.7425 - val_loss: 0.5089 - val_accuracy: 0.7453\n",
      "Epoch 42/50\n",
      "456/456 - 8s - loss: 0.5105 - accuracy: 0.7429 - val_loss: 0.5108 - val_accuracy: 0.7338\n",
      "Epoch 43/50\n",
      "456/456 - 9s - loss: 0.5107 - accuracy: 0.7426 - val_loss: 0.5087 - val_accuracy: 0.7423\n",
      "Epoch 44/50\n",
      "456/456 - 9s - loss: 0.5106 - accuracy: 0.7428 - val_loss: 0.5098 - val_accuracy: 0.7470\n",
      "Epoch 45/50\n",
      "456/456 - 8s - loss: 0.5108 - accuracy: 0.7429 - val_loss: 0.5087 - val_accuracy: 0.7453\n",
      "Epoch 46/50\n",
      "456/456 - 8s - loss: 0.5105 - accuracy: 0.7425 - val_loss: 0.5088 - val_accuracy: 0.7453\n",
      "Epoch 47/50\n",
      "456/456 - 8s - loss: 0.5107 - accuracy: 0.7427 - val_loss: 0.5088 - val_accuracy: 0.7397\n",
      "Epoch 48/50\n",
      "456/456 - 9s - loss: 0.5105 - accuracy: 0.7427 - val_loss: 0.5087 - val_accuracy: 0.7446\n",
      "Epoch 49/50\n",
      "456/456 - 8s - loss: 0.5106 - accuracy: 0.7429 - val_loss: 0.5094 - val_accuracy: 0.7434\n",
      "Epoch 50/50\n",
      "456/456 - 8s - loss: 0.5106 - accuracy: 0.7429 - val_loss: 0.5087 - val_accuracy: 0.7427\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 50     # number of epochs to train on\n",
    "batch_size = 512 # training batch size\n",
    "filepath = 'E:\\\\projects\\\\plds_teamc\\\\data\\\\HARRY.h5'\n",
    "\n",
    "history = model.fit(X_train,\n",
    "    Y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=nb_epoch,\n",
    "    verbose=2,\n",
    "    validation_data=(X_test,Y_test),\n",
    "    shuffle=True)\n",
    "\n",
    "\n",
    "# convert the history.history dict to a pandas DataFrame:   \n",
    "# #https://stackoverflow.com/questions/41061457/keras-how-to-save-the-training-history-attribute-of-the-history-object  \n",
    "hist_df = pd.DataFrame(history.history) \n",
    "hist_csv_file = 'E:\\\\projects\\\\plds_teamc\\\\data\\\\history.csv'\n",
    "with open(hist_csv_file, mode='w') as f:\n",
    "    hist_df.to_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E:\\\\projects\\\\plds_teamc\\\\data\\\\acc_threshold.pkl']"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_threshold = hist_df[\"accuracy\"].mean()\n",
    "joblib.dump(acc_threshold, 'E:\\\\projects\\\\plds_teamc\\\\data\\\\acc_threshold.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = KNeighborsClassifier(n_neighbors=50)\n",
    "#base_model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "#base_model = MLPClassifier(hidden_layer_sizes=(100,300,150), random_state=42, max_iter=300)\n",
    "\n",
    "\n",
    "model_fitted = base_model.fit(X_train, Y_train)\n",
    "\n",
    "#auc_score = roc_auc_score(Y_test, model_fitted.predict(X_test))\n",
    "auc_score = roc_auc_score(Y_test, model_fitted.predict(X_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Patient Age at Treatment                                         2\n",
       "Total Number of Previous cycles, Both IVF and DI                 2\n",
       "Total number of IVF pregnancies                                  0\n",
       "Total number of live births - conceived through IVF              0\n",
       "Type of Infertility - Female Primary                             0\n",
       "Type of Infertility - Female Secondary                           0\n",
       "Type of Infertility - Male Primary                               0\n",
       "Type of Infertility - Male Secondary                             0\n",
       "Type of Infertility -Couple Primary                              0\n",
       "Type of Infertility -Couple Secondary                            0\n",
       "Cause  of Infertility - Tubal disease                            0\n",
       "Cause of Infertility - Ovulatory Disorder                        0\n",
       "Cause of Infertility - Male Factor                               0\n",
       "Cause of Infertility - Patient Unexplained                       0\n",
       "Cause of Infertility - Endometriosis                             0\n",
       "Cause of Infertility - Cervical factors                          0\n",
       "Cause of Infertility - Female Factors                            0\n",
       "Cause of Infertility - Partner Sperm Concentration               0\n",
       "Cause of Infertility -  Partner Sperm Morphology                 0\n",
       "Causes of Infertility - Partner Sperm Motility                   0\n",
       "Cause of Infertility -  Partner Sperm Immunological factors      0\n",
       "Stimulation used                                                 1\n",
       "Egg Source                                                       1\n",
       "Sperm From                                                       0\n",
       "Fresh Cycle                                                    1.0\n",
       "Frozen Cycle                                                   0.0\n",
       "Eggs Mixed With Partner Sperm                                    0\n",
       "Eggs Thawed                                                    0.0\n",
       "Embryos Transfered                                             1.0\n",
       "Fresh Eggs Collected                                            13\n",
       "Name: 268431, dtype: object"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, Y_train,  Y_test= load_dataset()\n",
    "text=X_train.iloc[6786]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: E:\\projects\\plds_teamc\\data\\assets\n"
     ]
    }
   ],
   "source": [
    "#joblib.dump(model, 'E:\\\\projects\\\\plds_teamc\\\\data\\\\mantab_model.pkl')\n",
    "model.save('E:\\\\projects\\\\plds_teamc\\\\data\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_predict(text, model, threshold):\n",
    "    text = text[[text.columns[1]]]\n",
    "    text = np.asarray(text).astype(np.float32).T\n",
    "    text = text.reshape(-1, 30)\n",
    "    code2rel = {0: 'Tidak berhasil', 1: 'Berhasil'}\n",
    "    \n",
    "    proba = model.predict(text)\n",
    "    predict = 1 if proba > threshold else 0\n",
    "    print(f\"{code2rel[predict]}, dengan akurasi {proba}\")\n",
    "\n",
    "    return code2rel[predict], proba\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('E:\\\\projects\\\\plds_teamc\\\\data\\\\')\n",
    "threshold = joblib.load('E:\\\\projects\\\\plds_teamc\\\\data\\\\acc_threshold.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pd.read_csv(\"E:\\\\projects\\\\plds_teamc\\\\data\\\\sample.txt\") \n",
    "#text = np.asarray(text).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tidak berhasil, dengan akurasi [[0.24795133]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "proba = main_predict(text, model, threshold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f9ce6eba129c4e9adeb7a9652474f34792e333d7d887e515c6f6483448857cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
